{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1VlUnh6cTaag/c7nyE0GV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poseidon2022/Retreival-Augumented-Generation/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iNnJxYXUwIY",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade fsspec==2024.6.1\n",
        "!pip install -qU \\\n",
        "  langchain==0.0.300 \\\n",
        "  datasets==2.14.6 \\\n",
        "  pinecone-client==2.2.4 \\\n",
        "  tiktoken==0.5.1\n",
        "!pip install langchain_google_genai\n",
        "!pip install pypdf==3.1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KE')\n",
        "PINECONE_API = userdata.get('PIN_CONE')\n",
        "PINECONE_ENV = 'gcp-starter'\n"
      ],
      "metadata": {
        "id": "cG2zvrKJbN0S"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\",\n",
        "                             google_api_key = GOOGLE_API_KEY)\n",
        "\n",
        "#Example generation\n",
        "llm.invoke(\"Write me a ballad about LangChain\").content"
      ],
      "metadata": {
        "id": "ukw3H70OdGkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nNXlqIHdc5Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#let us have a continuous in-memory context remembering example\n",
        "messages = [\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"human\", \"Hi AI how are you today?\"),\n",
        "    (\"ai\", \"I am great, how can I help you today?\"),\n",
        "    (\"human\", \"I want to know more about string theory.\")\n",
        "]"
      ],
      "metadata": {
        "id": "fYCQ3w3xewrc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = llm.invoke(messages)\n",
        "res.content"
      ],
      "metadata": {
        "id": "-7B1p-JJgxqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append((\"ai\", res.content))\n",
        "messages"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9o3oK-y9iOv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append((\"human\", \"tell me more\"))\n",
        "llm.invoke(messages).content"
      ],
      "metadata": {
        "id": "nwmRS8GIj14X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have our llm setup and the next thing is loading up a freash document. I setup a document on a fictional company named Kidus Melaku Simegne. But you can load up any document and test the functionality."
      ],
      "metadata": {
        "id": "4QAoDEXjkyRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "import uuid\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",\n",
        "                                          google_api_key = GOOGLE_API_KEY)\n",
        "#example embedding\n",
        "vector = embeddings.embed_query(\"hello, world!\")\n",
        "vector[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwNQ7ZRFodvO",
        "outputId": "c63a183e-09d7-46aa-c981-e00a3cad3201"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05168594419956207,\n",
              " -0.030764883384108543,\n",
              " -0.03062233328819275,\n",
              " -0.02802734449505806,\n",
              " 0.01813092641532421]"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we have to load up our document and try to split it into smalled chunks that\n",
        "#are later to be used for embedding\n",
        "def load_document(file_path):\n",
        "  loader = PyPDFLoader(file_path)\n",
        "  document = loader.load()\n",
        "\n",
        "  #now we use a recursive text splitter to chunk up the text into a list of smaller texts\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=50\n",
        "  )\n",
        "  chunks = text_splitter.split_documents(document)\n",
        "\n",
        "  #now let us extraxt the content from the chunks and use google's generative ai embedding\n",
        "  #to prepare our embedding vector\n",
        "  batch_size = 20\n",
        "  ids = []\n",
        "  context_array = []\n",
        "  meta_data = []\n",
        "  for i in tqdm(range(0, len(chunks), batch_size)):\n",
        "    i_end = min(i + batch_size, len(chunks))\n",
        "    batch = chunks[i:i_end]\n",
        "    for idx, row in enumerate(batch):\n",
        "      print(f\"appending {idx + i}\")\n",
        "      ids.append(str(uuid.uuid4()))\n",
        "      context_array.append(batch[idx].page_content)\n",
        "      meta_data.append({\n",
        "          'source' : row.metadata[\"source\"],\n",
        "          'page' : row.metadata[\"page\"] + 1,\n",
        "          'context' : batch[idx].page_content\n",
        "      })\n",
        "\n",
        "\n",
        "  emb_vectors = embeddings.embed_documents(context_array)\n",
        "  return ids, emb_vectors, meta_data\n",
        "\n",
        "\n",
        "#let us check if our embedding function is working correctly\n",
        "ids, emb_vectors, meta_data = load_document(\"Kidus Melaku's company.pdf\")\n",
        "len(emb_vectors)"
      ],
      "metadata": {
        "id": "AH5e2uLppwGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have our embeding function and to insert any embeded vector along with its meta data and id to a database, we use a vector database known as pinecone for that. we will create an index with a specified dimension and we will try to upsert the embeded document to our database"
      ],
      "metadata": {
        "id": "1gr3CrkL0FKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "\n",
        "#setting up the index\n",
        "pinecone.init(api_key = PINECONE_API, environment = PINECONE_ENV)\n",
        "index_list = pinecone.list_indexes() #this is to make sure that the same index is not\n",
        "#created multiple times when running the application multiple times.\n",
        "\n",
        "if not index_list:\n",
        "  print(\"Creating index\")\n",
        "  pinecone.create_index(\"default\", dimension = 768, metric = \"cosine\")\n",
        "\n",
        "#One can change the metric to dotproduct and anu other, based on the application\n",
        "\n",
        "#this is to make sure that our index is created with the dimensions defined and all\n",
        "print(pinecone.describe_index(\"default\"))\n",
        "index = pinecone.Index(\"default\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvP1O7kZ33OE",
        "outputId": "4356606b-82f0-4316-fb29-9e7fcafefd1f"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating index\n",
            "IndexDescription(name='default', metric='cosine', replicas=1, dimension=768.0, shards=1, pods=1, pod_type='starter', status={'ready': True, 'state': 'Ready'}, metadata_config=None, source_collection='')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now let us create our upserting function for the embeded vector to pinecone\n",
        "\n",
        "def upsert(ids, emb_vectors, meta_data, index):\n",
        "   to_upsert = zip(ids, emb_vectors, meta_data)\n",
        "   index.upsert(vectors = to_upsert)\n",
        "   time.sleep(2)\n"
      ],
      "metadata": {
        "id": "o2szMkbo5voz"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let us tru upserting a sample document to the vector database\n",
        "ids, emb_vectors, meta_data = load_document(\"Kidus Melaku's company.pdf\")\n",
        "upsert(ids, emb_vectors, meta_data, index)"
      ],
      "metadata": {
        "id": "DYYViJGT6k5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.describe_index_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGxC6-Mz-gok",
        "outputId": "4aa6ebea-f84a-4494-f83c-d169771ca426"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 768,\n",
              " 'index_fullness': 0.00119,\n",
              " 'namespaces': {'': {'vector_count': 119}},\n",
              " 'total_vector_count': 119}"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now let us initialize our vector store from here\n",
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = \"context\"\n",
        "\n",
        "vectorstore = Pinecone(index, embeddings.embed_query,text_field)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHWV7SYzKiGk",
        "outputId": "cd22cf6b-c19c-40dd-94d9-d8276b7bdb6c"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/pinecone.py:59: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exampl match from our vector database\n",
        "query = \"what software technologies does the company offer\"\n",
        "vectorstore.similarity_search(query, k = 3)"
      ],
      "metadata": {
        "id": "FIG44dWiL_Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augument_prompt(query:str):\n",
        "  results = vectorstore.similarity_search(query, k=5)\n",
        "\n",
        "  #the source knowledge to provide to our llm\n",
        "  source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
        "\n",
        "  augumented_prompt = f\"\"\"using the contexts below answer the query.\n",
        "\n",
        "  Contexts: {source_knowledge}\n",
        "\n",
        "  Query : {query}\"\"\"\n",
        "\n",
        "  return augumented_prompt\n",
        "\n",
        "\n",
        "print(augument_prompt(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzy9uxgjMwCO",
        "outputId": "badc2834-19f2-4bed-c413-b01c15909bbe"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using the contexts below answer the query.\n",
            "\n",
            "  Contexts: ever-changing technology landscape. Whether it’s developing an IT strategy, optimizing existing \n",
            "systems, or implementing new technologies, the company’s consultants have the knowledge and\n",
            "Simegne has the expertise and experience to deliver high-quality software that meets and \n",
            "exceeds client expectations. \n",
            "Sub-services under Software Development :\n",
            "areas: \n",
            "1. Software Development \n",
            "At the core of Kidus Melaku Simegne’s offerings is its software development service. The\n",
            "assurance services to ensure the reliability and performance of software products. \n",
            "2. Artificial Intelligence and Machine Learning\n",
            "Sub-services under Software Development : \n",
            " Custom Software Development : Designing and developing software solutions from \n",
            "scratch, tailored to the unique requirements of each client.\n",
            "\n",
            "  Query : what software technologies does the company offer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to create a new prompt and pass it to the large language model. this is the generation step and we should observe the LLM perform better than it did without the RAG implementation."
      ],
      "metadata": {
        "id": "xpLWVo8EPI9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"human\", \"Hi AI how are you today?\"),\n",
        "    (\"ai\", \"I am great, how can I help you today?\"),\n",
        "]\n",
        "\n",
        "\n",
        "query = \"What is Kidus Melaku simegne\"\n",
        "messages.append((\"human\", augument_prompt(query)))\n",
        "response = llm.invoke(messages).content\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "r3gDNdpOP7JV",
        "outputId": "4434ae40-c66c-48df-9942-22176f8044c1"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the provided text, Kidus Melaku Simegne appears to be a company (\"...Kidus Melaku Simegne stands as a beacon of excellence in the digital services industry.\"). \\n\\nFurthermore, it is:\\n\\n* **A digital services company:**  Offering a range of services including software development and artificial intelligence solutions.\\n* **Focused on emerging technologies:**  Specializing in AI, machine learning, and data analytics.\\n* **Committed to customer satisfaction:**  Prioritizing a seamless experience for its customers.\\n* **Research and development oriented:**  Continuously investing in R&D to stay at the forefront of technology.\\n* **Ambitious and growth-oriented:**  With plans for future expansion. \\n\\nThe text doesn\\'t explicitly state what Kidus Melaku Simegne is, but it heavily implies it\\'s the name of the company being described. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    }
  ]
}